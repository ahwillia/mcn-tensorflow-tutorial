{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with TensorFlow\n",
    "\n",
    "This notebook contains some small code pieces to give you a sense of basic tensorflow functions. You can use this as a space to play around with tensorflow functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session\n",
    "\n",
    "The code below demonstrates how to use `sess.run()` to make tensorflow talk to python and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=string)\n",
      "b'hello world!'\n",
      "--------------------------------------------------\n",
      "Tensor(\"Const_1:0\", shape=(4,), dtype=int32)\n",
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# an example with strings\n",
    "hello_tensorflow = tf.constant('hello world!')\n",
    "print(hello_tensorflow)\n",
    "\n",
    "hello_python = sess.run(hello_tensorflow)\n",
    "print(hello_python), print('-'*50)\n",
    "\n",
    "# an example with numbers\n",
    "tensorflow_vector = tf.constant([1, 2, 3, 4])\n",
    "print(tensorflow_vector)\n",
    "\n",
    "python_vector = sess.run(tensorflow_vector)\n",
    "print(python_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders\n",
    "\n",
    "The code below demonstrates how to use `feed_dict` in conjunction with placeholder variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  6.], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make two placeholders\n",
    "foo = tf.placeholder(tf.float32, shape=(2,))\n",
    "bar = tf.placeholder(tf.float32, shape=(2,))\n",
    "\n",
    "# computational graph\n",
    "baz = foo * bar\n",
    "\n",
    "# values for the placeholders\n",
    "feed_dict = {\n",
    "    foo: np.array([1, 2]),\n",
    "    bar: np.array([2, 3])\n",
    "}\n",
    "\n",
    "sess.run(baz, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients\n",
    "\n",
    "You can use [`tf.gradients(y, x)`](https://www.tensorflow.org/api_docs/python/tf/gradients) to compute the derivative of a variable `y` with respect to `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deriv of x1 w.r.t. loss = 1.0\n",
      "deriv of w1 w.r.t. loss = 1.0\n",
      "deriv of w2 w.r.t. loss = 1.0\n",
      "deriv of y w.r.t. loss = -1.0\n"
     ]
    }
   ],
   "source": [
    "x1 = tf.placeholder(tf.float32)\n",
    "w1 = tf.placeholder(tf.float32)\n",
    "w2 = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "x2 = x1 * w1\n",
    "x3 = x2 * w2\n",
    "\n",
    "loss = 0.5 * (y - x3)**2\n",
    "\n",
    "feed_dict = {\n",
    "    x1: 1,\n",
    "    w1: 1,\n",
    "    w2: 1,\n",
    "    y: 0\n",
    "}\n",
    "\n",
    "for variable, name in zip([x1, w1, w2, y], ['x1', 'w1', 'w2', 'y']):\n",
    "    val = sess.run(tf.gradients(loss, variable), feed_dict=feed_dict)[0]\n",
    "    print('deriv of {} w.r.t. loss = {}'.format(name, val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the whole point is that `tensorflow` makes it easy to scale up the above example to large matrix multiplications and nonlinear functions. Here is a very simple three layer feedforward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deriv of w1 w.r.t. loss = [[ 0.56114584  1.70765328  0.43764409 ...,  1.54541659  0.04804779\n",
      "   0.08327151]\n",
      " [ 0.68632495  2.08859253  0.53527272 ...,  1.89016449  0.05876618\n",
      "   0.10184753]\n",
      " [-2.98942351 -9.09727669 -2.33148575 ..., -8.23298359 -0.25596768\n",
      "  -0.44361699]\n",
      " ..., \n",
      " [ 2.41859221  7.36014938  1.88628793 ...,  6.66089344  0.20709059\n",
      "   0.35890821]\n",
      " [ 1.60476017  4.88353252  1.25157094 ...,  4.41956949  0.13740668\n",
      "   0.23813918]\n",
      " [-1.64216673 -4.99736643 -1.28074467 ..., -4.52258825 -0.14060959\n",
      "  -0.24369015]]\n",
      "deriv of w2 w.r.t. loss = [[  0.2368754   -0.02027308  -0.02844641 ...,  -0.1672783   -0.13502629\n",
      "    0.20465143]\n",
      " [ -0.96353322   0.08246438   0.11571088 ...,   0.68043453   0.54924363\n",
      "   -0.83245647]\n",
      " [ -4.83959532   0.41419876   0.5811879  ...,   3.41765881   2.75871873\n",
      "   -4.18122864]\n",
      " ..., \n",
      " [  7.24715424  -0.62025064  -0.87031204 ...,  -5.11784554  -4.13110161\n",
      "    6.26126862]\n",
      " [-15.03609371   1.28687024   1.80568707 ...,  10.61829185   8.57103729\n",
      "  -12.99061871]\n",
      " [ 13.47215939  -1.15302026  -1.61787403 ...,  -9.51386261  -7.67954683\n",
      "   11.63943863]]\n"
     ]
    }
   ],
   "source": [
    "# weights between layers 1 -> 2 and layers 2 -> 3\n",
    "w1 = tf.Variable(np.random.randn(100, 200), dtype=tf.float32)\n",
    "w2 = tf.Variable(np.random.randn(200, 10), dtype=tf.float32)\n",
    "\n",
    "# activation of each layer\n",
    "x1 = tf.placeholder(tf.float32, shape=(None, 100))\n",
    "x2 = tf.nn.tanh(tf.matmul(x1, w1))\n",
    "x3 = tf.matmul(x2, w2)\n",
    "\n",
    "# Note the shape of `x` has a \"None\" in it. This tells tensorflow that\n",
    "# the shape of that dimension will be determined at runtime, which can be\n",
    "# a handy trick when you aren't sure what the dimensions of your data are.\n",
    "\n",
    "# loss function\n",
    "target = tf.placeholder(tf.float32, shape=10)\n",
    "loss = tf.nn.l2_loss(x3 - target)\n",
    "\n",
    "# initialize all variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# define the input and target for the network. You can try changing these\n",
    "# values if you like.\n",
    "feed_dict = {\n",
    "    x1: np.random.randn(1, 100)*.1,\n",
    "    target: np.random.randn(10)\n",
    "}\n",
    "\n",
    "# compute gradients\n",
    "for variable, name in zip([w1, w2], ['w1', 'w2']):\n",
    "    val = sess.run(tf.gradients(loss, variable), feed_dict=feed_dict)[0]\n",
    "    print('deriv of {} w.r.t. loss = {}'.format(name, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
